{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TASK 1: Defect Analysis",
   "id": "c6934408fc105b3e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ],
   "id": "97a63fb0c21ca780"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "KEYWORDS = [\"fix\", \"bug\", \"error\", \"issue\", \"hotfix\", \"resolve\", \"repair\"]",
   "id": "41375393b4382ed0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Obtention of commits since a given date (2023-01-01)",
   "id": "2b14aa8a55e5d880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_git_commits_since(date=\"2023-01-01\"):\n",
    "    \"\"\"\n",
    "    Gets git commits since a given date using git log.\n",
    "    \"\"\"\n",
    "    git_command = [\n",
    "        \"git\", \"log\",\n",
    "        f'--since={date}',\n",
    "        \"--pretty=format:%H|%ad|%s\",\n",
    "        \"--date=short\",\n",
    "        \"--name-only\"\n",
    "    ]\n",
    "    result = subprocess.run(git_command, capture_output=True, text=True, encoding=\"utf-8\", check=True)\n",
    "    return result.stdout.split(\"\\n\") #we split the result in lines to process it more easily"
   ],
   "id": "588512a981a4b21b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Creation of a commit dictionary (from unstructured raw git output to structured data)",
   "id": "8bc49886c2df7395"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_commits(raw_lines):\n",
    "    \"\"\"\n",
    "    Convert the raw lines returned by get_commits_since()\n",
    "    into a clean list of commit dictionaries.\n",
    "    Each commit contains: hash, date, message, and the files touched.\n",
    "    \"\"\"\n",
    "    commits = []\n",
    "    current = {}\n",
    "\n",
    "    for line in raw_lines:\n",
    "        if \"|\" in line: # a header line contains hash | date | message\n",
    "            if current:\n",
    "                commits.append(current)\n",
    "            hash_, date, msg = line.split(\"|\", 2)\n",
    "            current = {\"hash\": hash_, \"date\": date, \"msg\": msg, \"files\": []}\n",
    "        elif line.strip() == \"\":\n",
    "            # empty line means a separator in git log output\n",
    "            continue\n",
    "        else:\n",
    "            # file modified in this commit\n",
    "            if \"files\" in current:\n",
    "                current[\"files\"].append(line.strip())\n",
    "\n",
    "    if current:\n",
    "        commits.append(current)\n",
    "\n",
    "    return commits"
   ],
   "id": "d86f5e9b7a798d00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Detection of defect commits",
   "id": "1328ffb2723c7aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def is_defect_commit(message):\n",
    "    \"\"\"\n",
    "    If the commit message contains any keyword that is considered as defect, it outputs True.\n",
    "    \"\"\"\n",
    "    msg = message.lower()\n",
    "    return any(k in msg for k in KEYWORDS)"
   ],
   "id": "a5ced32577c39f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Computation of defects per month, defects per file and defects per month for a file",
   "id": "c2d58cc5a5eec725"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def defects_per_month(commits):\n",
    "    \"\"\"\n",
    "    Counts how many defect commits happened in each month, and returns a dict of the form: { 'YYYY-MM' : number_of_defects }\n",
    "    \"\"\"\n",
    "    counter = defaultdict(int)\n",
    "    for c in commits:\n",
    "        # only count commits whose message suggests a defect fix\n",
    "        if is_defect_commit(c[\"msg\"]):\n",
    "            month = c[\"date\"][:7] # extracting YYYY-MM\n",
    "            counter[month] += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "def defects_per_file(commits):\n",
    "    \"\"\"\n",
    "    Counts how many times a file appeared in a defect commit. Returns a dict: { 'path/to/file.py' : number_of_defects }\n",
    "    \"\"\"\n",
    "    file_counts = defaultdict(int)\n",
    "    for c in commits:\n",
    "        if is_defect_commit(c[\"msg\"]):\n",
    "            #every modified file in this commit is counted\n",
    "            for f in c[\"files\"]:\n",
    "                file_counts[f] += 1\n",
    "    return file_counts\n",
    "\n",
    "\n",
    "def defects_per_month_for_file(commits, target_file):\n",
    "    \"\"\"\n",
    "    Counts defect commits for a specific file, group by month. Returns a dict: { 'YYYY-MM' : number_of_defects_for_that_file }\n",
    "    \"\"\"\n",
    "    counter = defaultdict(int)\n",
    "    for c in commits:\n",
    "        # only count defect commits that touched this specific file\n",
    "        if is_defect_commit(c[\"msg\"]) and target_file in c[\"files\"]:\n",
    "            month = c[\"date\"][:7]\n",
    "            counter[month] += 1\n",
    "    return counter\n"
   ],
   "id": "57b5f36ead97dc45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_month_series(commits, defects_by_month):\n",
    "    \"\"\"\n",
    "    Creates a month-by-month series based on all commit dates,\n",
    "    filling missing months with 0 defects. \n",
    "    \"\"\"\n",
    "    all_months = sorted({commit[\"date\"][:7] for commit in commits}) # we extract all months that appear in the commit history\n",
    "\n",
    "    full_series = {month: defects_by_month.get(month, 0) for month in all_months} # we build a complete series, using 0 when a month has no defects. This is useful for ploting all months.\n",
    "    return full_series"
   ],
   "id": "230bba84c208c7e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Plotting utils",
   "id": "b9f0509aa63a8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_bar_chart(data, title, ylabel):\n",
    "\n",
    "    months = sorted(data.keys()) # we sort keys to ensure the bars appear in chronological order\n",
    "    values = [data[m] for m in months]\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 5) \n",
    "    plt.rcParams[\"figure.dpi\"] = 120\n",
    "    plt.figure()\n",
    "    plt.bar(months, values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "e7a1292de5723fd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Run Task 1 Analysis",
   "id": "b139e325c79e9bec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Obtaining commits sice 2023-01-01\n",
    "raw = get_git_commits_since()\n",
    "commits = parse_commits(raw)\n",
    "len(commits)"
   ],
   "id": "31ec17d5adc1783d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Total number of defects per month\n",
    "dpm = defects_per_month(commits)\n",
    "dpm_full = build_month_series(commits, dpm)\n",
    "print(\"Defects per month:\", dpm_full)\n",
    "plot_bar_chart(dpm_full, \"Defects per Month\", \"Number of Defects\")"
   ],
   "id": "c70b0cb15fd4ac7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Top 2 defective files\n",
    "df = defects_per_file(commits)\n",
    "top2 = sorted(df.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "top2"
   ],
   "id": "b399f9c9a2a854ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Defects per month for each of the most defective files\n",
    "for f, count in top2:\n",
    "    print(f\"\\nFile: {f} (total defects: {count})\")\n",
    "    dpf = defects_per_month_for_file(commits, f)\n",
    "    print(\"Defects per month for this file:\", dpf)\n",
    "    full_counts = build_month_series(commits, dpf)\n",
    "    plot_bar_chart(full_counts, f\"Defects per Month for {f}\", \"Number of Defects\")"
   ],
   "id": "dceb5fd376f83c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 2: Complexity Analysis",
   "id": "afab39b86de185c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup transformers repository",
   "id": "a81e092417085a6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:16:13.141094Z",
     "start_time": "2025-11-24T22:16:13.137495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_url = \"https://github.com/huggingface/transformers\"\n",
    "\n",
    "if not Path(\"transformers\").exists():\n",
    "    print(\"Cloning the Transformers repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)"
   ],
   "id": "5817295fa51a3655",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Implement task",
   "id": "824af1ef5c2e1a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:18:22.700091Z",
     "start_time": "2025-11-24T22:18:22.609624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def dict_to_markdown_table(input_data: dict[str, int]) -> str:\n",
    "    header = \"| Count | File Name |\\n\"\n",
    "    separator = \"|-----------|------------|\\n\"\n",
    "    rows = \"\"\n",
    "\n",
    "    for key, value in input_data.items():\n",
    "        rows += f\"| {value} | {key} |\\n\"\n",
    "\n",
    "    return header + separator + rows\n",
    "\n",
    "\n",
    "def count_commits_in_repo(repo_path: Path, since_date: str) -> dict[str, int]:\n",
    "    result = {}\n",
    "    command = f\"git log --since='{since_date}' --pretty=format: --name-only\"\n",
    "\n",
    "    os.chdir(repo_path)\n",
    "    output = subprocess.check_output(command, shell=True, text=True)\n",
    "    files = output.strip().splitlines()\n",
    "\n",
    "    for file in files:\n",
    "        if file and file.endswith('.py'):\n",
    "            result[file] = result.get(file, 0) + 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def count_lines_of_python_files(directory: Path, exclude_venv: bool = True, exclude_empty_init: bool = True) -> dict[str, int]:\n",
    "    result = {}\n",
    "\n",
    "    if not directory.exists():\n",
    "        print(f\"ERROR: Directory {directory} does not exist\")\n",
    "        return result\n",
    "\n",
    "    for file in directory.glob(\"**/*.py\"):\n",
    "        lines_count = len(file.open(\"r\", encoding='utf-8').readlines())\n",
    "        relative_file_path = file.relative_to(directory).__str__()\n",
    "\n",
    "        if exclude_venv and relative_file_path.startswith(\".venv\"):\n",
    "            continue\n",
    "        elif exclude_empty_init and file.name == \"__init__.py\" and lines_count == 0:\n",
    "            continue\n",
    "\n",
    "        result[relative_file_path] = lines_count\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_bar_chart(input_data: dict[str, int], show_top_n_entries: int = 30) -> None:\n",
    "    top_files = sorted(input_data.items(), key=lambda item: item[1], reverse=True)[:show_top_n_entries]\n",
    "    file_names, line_counts = zip(*top_files)\n",
    "\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    plt.barh(file_names, line_counts, color='skyblue')\n",
    "    plt.title(f\"Top {show_top_n_entries} Files by Line Count\")\n",
    "    plt.xlabel(\"Line Count\")\n",
    "    plt.ylabel(\"File Name\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis=\"x\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def create_word_cloud(input_data: dict[str, int], width: int = 1080, height: int = 720) -> None:\n",
    "    wordcloud = WordCloud(width=width, height=height, background_color=\"white\")\n",
    "    wordcloud.generate_from_frequencies(input_data)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir('/')\n",
    "\n",
    "    repo_path = Path(\"transformers\")\n",
    "    figures_output_path = Path(\"images\")\n",
    "\n",
    "    # Count lines of code\n",
    "    commit_counts = count_commits_in_repo(repo_path, \"2023-01-01\")\n",
    "    sorted_commit_counts = dict(sorted(commit_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    markdown_table = dict_to_markdown_table(sorted_commit_counts)\n",
    "    Path(\"docs/commits_count_table.md\").open(\"w\", encoding=\"utf-8\").write(markdown_table)\n",
    "\n",
    "    # Count commits\n",
    "    line_counts_total = count_lines_of_python_files(repo_path, exclude_venv=False, exclude_empty_init=False)\n",
    "    line_counts_without_venv_and_empty_init_files = count_lines_of_python_files(repo_path)\n",
    "    line_counts_test_module = {}\n",
    "    line_counts_src_module = {}\n",
    "\n",
    "    for key, value in line_counts_without_venv_and_empty_init_files.items():\n",
    "        if key.startswith(\"tests\"):\n",
    "            line_counts_test_module[key.replace(\"tests/\", \"\")] = value\n",
    "        elif key.startswith(\"src/transformers\"):\n",
    "            line_counts_src_module[key.replace(\"src/transformers/\", \"\")] = value\n",
    "\n",
    "    sorted_line_counts_total = dict(sorted(line_counts_total.items(), key=lambda item: item[1], reverse=True))\n",
    "    markdown_table = dict_to_markdown_table(sorted_line_counts_total)\n",
    "    Path(\"docs/line_count_table.md\").open(\"w\", encoding=\"utf-8\").write(markdown_table)\n",
    "\n",
    "    create_bar_chart(line_counts_src_module, figures_output_path)\n",
    "    create_bar_chart(line_counts_test_module, figures_output_path)\n",
    "    create_word_cloud(line_counts_src_module, figures_output_path)\n",
    "    create_word_cloud(line_counts_test_module, figures_output_path)\n",
    "\n",
    "    # use both metrics\n",
    "    top_n = 10\n",
    "\n",
    "    commit_counts = count_commits_in_repo(repo_path, \"2023-01-01\")\n",
    "    sorted_commit_counts = sorted(commit_counts.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "\n",
    "    line_counts = count_lines_of_python_files(repo_path)\n",
    "    sorted_line_counts = sorted(line_counts.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "\n",
    "    header = \"| Top n | File Name | Commits / Lines of Code (LoC) | Bar |\\n\"\n",
    "    separator = \"|-----------|-----------|-----------|------------|\\n\"\n",
    "    rows = header + separator\n",
    "    for i in range(top_n):\n",
    "        commit_count_file_name = sorted_commit_counts[i][0]\n",
    "        commit_count = sorted_line_counts[i][1]\n",
    "\n",
    "        lines_count_file_name = sorted_line_counts[i][0]\n",
    "        lines_count = sorted_commit_counts[i][1]\n",
    "\n",
    "        bar = f\"*\" * (commit_count // 70)\n",
    "        rows += f\"| {i + 1} | {commit_count_file_name} | Commits: {commit_count:,} | {bar} |\\n\"\n",
    "\n",
    "        bar = f\"*\" * (lines_count // 70)\n",
    "        rows += f\"| {i + 1} | {lines_count_file_name} | LoC: {lines_count:,} | {bar} |\\n\"\n",
    "\n",
    "    Path(\"../docs/LoC_and_NCC.md\").open(\"w\", encoding=\"utf-8\").write(rows)"
   ],
   "id": "9c961e7ef01e1ea8",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 86\u001B[39m\n\u001B[32m     83\u001B[39m figures_output_path = Path(\u001B[33m\"\u001B[39m\u001B[33mimages\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     85\u001B[39m \u001B[38;5;66;03m# Count lines of code\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m commit_counts = \u001B[43mcount_commits_in_repo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m2023-01-01\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     87\u001B[39m sorted_commit_counts = \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28msorted\u001B[39m(commit_counts.items(), key=\u001B[38;5;28;01mlambda\u001B[39;00m item: item[\u001B[32m1\u001B[39m], reverse=\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[32m     88\u001B[39m markdown_table = dict_to_markdown_table(sorted_commit_counts)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mcount_commits_in_repo\u001B[39m\u001B[34m(repo_path, since_date)\u001B[39m\n\u001B[32m     20\u001B[39m result = {}\n\u001B[32m     21\u001B[39m command = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mgit log --since=\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msince_date\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m --pretty=format: --name-only\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepo_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m output = subprocess.check_output(command, shell=\u001B[38;5;28;01mTrue\u001B[39;00m, text=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     25\u001B[39m files = output.strip().splitlines()\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'transformers'"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TASK 3: Coupling Analysis",
   "id": "b8dd8a869101e9f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "a7538dc8c599c4e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Computation of logical coupling for all Python file pairs",
   "id": "741c497ad7121000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def is_python_file(path):\n",
    "    return path.endswith(\".py\") # check to keep only Python source files.\n",
    "\n",
    "def compute_logical_coupling(commits):\n",
    "    coupling_counts = defaultdict(int)\n",
    "\n",
    "    for commit in commits:\n",
    "        python_files = [f for f in commit[\"files\"] if is_python_file(f)] # we keep only Python files for this commit\n",
    "\n",
    "        \n",
    "        unique_files = sorted(set(python_files)) # we use a set to avoid counting the same file twice in one commit\n",
    "\n",
    "        # For each unordered pair of files, we increase the coupling count\n",
    "        for file_a, file_b in itertools.combinations(unique_files, 2):\n",
    "            pair = (file_a, file_b)\n",
    "            coupling_counts[pair] += 1\n",
    "\n",
    "    return coupling_counts"
   ],
   "id": "759d0189b2091027"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Plot of the top 10 coupled pairs ",
   "id": "649b3a7e00295f42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_top_coupled_pairs(coupling_counts, top_n=10, title=\"Top coupled file pairs\"):\n",
    "\n",
    "    \n",
    "    top_pairs = sorted(coupling_counts.items(), key=lambda item: item[1], reverse=True)[:top_n] # we sort pairs by co-change count in descending order \n",
    "\n",
    "    pair_labels = [f\"{a}\\n{b}\" for (a, b), _ in top_pairs]\n",
    "    cochange_values = [count for _, count in top_pairs]\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=120)\n",
    "    plt.barh(range(len(top_pairs)), cochange_values)\n",
    "    plt.yticks(range(len(top_pairs)), pair_labels)\n",
    "    plt.xlabel(\"Number of co-changes\")\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis() \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return top_pairs \n",
    "\n",
    "# we run the analysis for all Python file pairs\n",
    "all_coupling = compute_logical_coupling(commits)\n",
    "top10_all = plot_top_coupled_pairs(all_coupling, top_n=10, title=\"Top 10 logically coupled Python file pairs\")\n",
    "top10_all"
   ],
   "id": "ccc9b9669a6032ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Computation of Logical Coupling restricted to test - non-test pairs",
   "id": "6d317ec7ffa244c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def is_test_file(path):\n",
    "    return is_python_file(path) and os.path.basename(path).startswith(\"test\") # we consider a python test file those files that start with \"test\" and end with .py\n",
    "\n",
    "def filter_test_non_test_pairs(coupling_counts):\n",
    "    filtered = {}\n",
    "\n",
    "    for (file_a, file_b), count in coupling_counts.items():\n",
    "        a_is_test = is_test_file(file_a)\n",
    "        b_is_test = is_test_file(file_b)\n",
    "\n",
    "        # we keep pairs where one is test and the other is non-test\n",
    "        if a_is_test ^ b_is_test:  # XOR-> exactly one True\n",
    "            filtered[(file_a, file_b)] = count\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "test_non_test_coupling = filter_test_non_test_pairs(all_coupling)\n",
    "\n",
    "top10_test_non_test = plot_top_coupled_pairs(\n",
    "    test_non_test_coupling,\n",
    "    top_n=10,\n",
    "    title=\"Top 10 test - non-test Python file pairs\"\n",
    ")\n",
    "\n",
    "top10_test_non_test"
   ],
   "id": "8d49ee95255f8857"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Method 1: Matching based on name:\n",
    "We find the most likely test file for a given source file based on naming patterns.\n",
    "    Examples:\n",
    "    - source: utils.py  ->  test_utils.py or utils_test.py\n",
    "    - source: trainer.py ->  test_trainer.py or trainer_test.py"
   ],
   "id": "23e15c88b770d482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, List"
   ],
   "id": "410fb4cb5e90c644"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_all_test_files(tests_root: Path) -> List[Path]:\n",
    "    return [p for p in tests_root.rglob(\"test*.py\") if p.is_file()] #we collect all Python test files under the given tests_root"
   ],
   "id": "8ec2a3d6d72cbb59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def find_test_by_name(source_file: Path, tests_root: Path) -> Optional[Path]:\n",
    "\n",
    "    source_name = source_file.stem \n",
    "\n",
    "    expected_names = {\n",
    "        f\"test_{source_name}.py\",\n",
    "        f\"{source_name}_test.py\",\n",
    "    }\n",
    "\n",
    "    candidates = []\n",
    "    for test_file in find_all_test_files(tests_root):\n",
    "        if test_file.name in expected_names:\n",
    "            candidates.append(test_file)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # If there are several, we just return the one with the shortest path (more specific)\n",
    "    return sorted(candidates, key=lambda p: len(p.as_posix()))[0]\n"
   ],
   "id": "5e66cee4bdef7e55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "project_root = Path(\".\").resolve()\n",
    "source_file = project_root / \"src/transformers/generation/utils.py\"\n",
    "tests_root = project_root / \"tests\"\n",
    "\n",
    "best_name_match = find_test_by_name(source_file, tests_root)\n",
    "best_name_match"
   ],
   "id": "c3bc3afc5eab08e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Method 2: Implement test finding method: find test through import statements",
   "id": "f3f92c624d5d1e84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_imports(file_path: Path) -> set:\n",
    "    node = ast.parse(file_path.open(\"r\").read(), filename=file_path)\n",
    "\n",
    "    imports = set()\n",
    "    for n in ast.walk(node):\n",
    "        if isinstance(n, ast.Import):\n",
    "            for alias in n.names:\n",
    "                imports.add(alias.name)\n",
    "        elif isinstance(n, ast.ImportFrom):\n",
    "            imports.add(n.module if n.module else '')\n",
    "\n",
    "    return imports\n",
    "\n",
    "\n",
    "def find_related_test_file(source_file: Path, test_directory: Path) -> Path | None:\n",
    "    source_imports = get_imports(source_file)\n",
    "    related_tests = {}\n",
    "\n",
    "    for file in test_directory.iterdir():\n",
    "        if file.name.endswith('_test.py') or file.stem.startswith('test_'):\n",
    "            test_file_path = file\n",
    "            test_imports = get_imports(test_file_path)\n",
    "\n",
    "            match_count = len(source_imports.intersection(test_imports))\n",
    "            if match_count > 0:\n",
    "                related_tests[test_file_path] = match_count\n",
    "\n",
    "    if related_tests:\n",
    "        return max(related_tests, key=related_tests.get)\n",
    "    return None\n",
    "\n",
    "\n",
    "test_source_file = Path('src/transformers/generation/utils.py')\n",
    "test_directory = Path('tests')\n",
    "related_test_file = find_related_test_file(test_source_file, test_directory)\n",
    "print(f'Related test file: {related_test_file}')"
   ],
   "id": "a44c3fd9e9659cfc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
