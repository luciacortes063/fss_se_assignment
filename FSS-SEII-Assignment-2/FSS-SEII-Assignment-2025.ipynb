{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fundamentals of Software Systems (FSS)\n",
    "**Software Evolution – Part 02 Assignment**\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "To correctly complete this assignment you must:\n",
    "* Carry out the assignment in a team of 2 to 4 students.\n",
    "* Carry out the assignment with your team only. You are allowed to discuss solutions with other teams, but each team should come up its own personal solution. A strict plagiarism policy is going to be applied to all the artifacts submitted for evaluation.\n",
    "* As your submission, upload the filled Jupyter Notebook (including outputs) together with the d3 visualization web pages (i.e. upload everything you downloaded including the filled Jupyter Notebook plus your `output.json`)\n",
    "* The files must be uploaded to OLAT as a single ZIP (`.zip`) file by Dec 15, 2025 @ 23:55.\n"
   ],
   "id": "7a85a628e359928b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Group Members\n",
    "* Firstname, Lastname, Immatrikulation Number\n",
    "* Lucía, Cortés Páez, 24-744-112\n",
    "* Oliver, Strassmann, 15-932-726"
   ],
   "id": "ab4e5b656408b052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task Context\n",
    "\n",
    "In this assigment we will be analyzing the _elasticsearch_ project. All following tasks should be done with the subset of commits from tag `v1.0.0` to tag `v1.1.0`."
   ],
   "id": "e49802c894594539"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T15:54:01.683785Z",
     "start_time": "2025-12-13T15:54:01.679272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "import pydriller\n",
    "\n",
    "\n",
    "class Modification(Enum):\n",
    "    ADDED = \"Lines added\"\n",
    "    REMOVED = \"Lines removed\"\n",
    "    TOTAL = \"Lines added + lines removed\"\n",
    "    DIFF = \"Lines added - lines removed\"\n",
    "\n",
    "\n",
    "# TODO: remove if not needed\n",
    "repo_url = \"https://github.com/elastic/elasticsearch\"\n",
    "repo = pydriller.Repository(repo_url, from_tag=\"v1.0.0\", to_tag=\"v1.1.0\")"
   ],
   "id": "dd4b6f3a0f58debd",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Author analysis\n",
    "\n",
    "In the following, please consider only `java` files.\n",
    "\n",
    "The first task is to get an overview of the author ownership of the _elasticsearch_ project. In particular, we want to understand who are the main authors in the system between the two considered tags, the authors distribution among files and the files distribution among authors. To this aim, perform the following:\n",
    "- create a dictionary (or a list of tuples) with the pairs author => number of modified files\n",
    "- create a dictionary (or a list of tuples) with the pairs file => number of authors who modified the file\n",
    "- visualize the distribution of authors among files: the visualization should have on the x axis the number of authors per file (from 1 to max), and on the y axis the number of files with the given number of authors (so for example the first bar represent the number of files with single author)\n",
    "- visualize the distribution of files among authors: the visualization should have on the x axis the number of files per author (from 1 to max), and on the y axis the number of authors that own the given number of files (so for example the first bar represent the minor contributors, i.e., the number of authors who own 1 file)\n",
    "\n",
    "Comment the two distribution visualizations.\n",
    "\n",
    "\n",
    "\n",
    "Now, let's look at the following 3 packages in more details:\n",
    "1. `src/main/java/org/elasticsearch/search`\n",
    "2. `src/main/java/org/elasticsearch/index`\n",
    "3. `src/main/java/org/elasticsearch/action`\n",
    "\n",
    "Create a function that, given the path of a package and a modification type (see class Modification above), returns a dictionary of authors => number, where the number counts the total lines added or removed or added+removed or added-removed (depending on the given Modification parameter), for the given package. To compute the value at the package level, you should aggregate the data per file.\n",
    "\n",
    "Using the function defined above, visualize the author contributions (lines added + lines removed). The visualization should have the author on the x axis, and the total lines on the y axis. Sort the visualization in decreasing amount of contributions, i.e., the main author should be the first.\n",
    "\n",
    "Compare the visualization for the 3 packages and comment."
   ],
   "id": "71764a87f9f21bb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T15:54:01.762294Z",
     "start_time": "2025-12-13T15:54:01.723872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from pydriller.git import Git\n",
    "\n",
    "repo_path = r\"C:\\Users\\lucia\\OneDrive\\Documentos\\Github_Repos\\elasticsearch\" #local repo path\n",
    "from_tag = \"v1.0.0\"\n",
    "to_tag = \"v1.1.0\"\n",
    "\n",
    "\n",
    "git = Git(repo_path) #we use GitPython directly (through PyDriller's Git wrapper) because traverse_commits() keeps returning 0 in our notebook\n",
    "repo = git.repo \n",
    "\n",
    "#we iterate commits in the tag range and collect java file ownership\n",
    "author_to_files = defaultdict(set)\n",
    "file_to_authors = defaultdict(set) \n",
    "\n",
    "commit_list = list(repo.iter_commits(f\"{from_tag}..{to_tag}\"))\n",
    "print(\"git commit count (from_tag..to_tag):\", len(commit_list))\n",
    "\n",
    "for c in commit_list:\n",
    "    author = c.author.name if c.author is not None else \"Unknown\"\n",
    "    touched_files = list(c.stats.files.keys()) #we use stats.files to get the list of touched files for this commit\n",
    "\n",
    "    for file_path in touched_files:\n",
    "        if not file_path.endswith(\".java\"): #we only consider java files\n",
    "            continue\n",
    "\n",
    "        author_to_files[author].add(file_path)\n",
    "        file_to_authors[file_path].add(author)\n",
    "\n",
    "print(\"unique java files touched:\", len(file_to_authors))\n",
    "print(\"unique authors touching java files:\", len(author_to_files))\n",
    "\n",
    "#we convert sets to counts\n",
    "author_to_num_files = {author: len(files) for author, files in author_to_files.items()}  # dictionary with the pairs author => number of modified files\n",
    "file_to_num_authors = {file_path: len(authors) for file_path, authors in file_to_authors.items()} # dictionary with the pairs file => number of authors who modified the file\n",
    "\n",
    "\n",
    "#visualization 1: distribution of authors among files\n",
    "authors_per_file = list(file_to_num_authors.values())\n",
    "authors_per_file_dist = Counter(authors_per_file)\n",
    "\n",
    "max_authors = max(authors_per_file_dist.keys())\n",
    "x = list(range(1, max_authors + 1))\n",
    "y = [authors_per_file_dist.get(k, 0) for k in x]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Number of authors per file\")\n",
    "plt.ylabel(\"Number of files\")\n",
    "plt.title(\"Authors among files\")\n",
    "plt.show()\n",
    "\n",
    "#visualization 2: distribution of files among authors\n",
    "files_per_author = list(author_to_num_files.values())\n",
    "files_per_author_dist = Counter(files_per_author)\n",
    "\n",
    "max_files = max(files_per_author_dist.keys())\n",
    "x = list(range(1, max_files + 1))\n",
    "y = [files_per_author_dist.get(k, 0) for k in x]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Number of files per author\")\n",
    "plt.ylabel(\"Number of authors\")\n",
    "plt.title(\"Files among authors \")\n",
    "plt.show()\n"
   ],
   "id": "bbdd76e9ab8ae977",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcollections\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Counter, defaultdict\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpydriller\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgit\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Git\n\u001B[32m      5\u001B[39m repo_path = \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mC:\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mUsers\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mlucia\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mOneDrive\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mDocumentos\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mGithub_Repos\u001B[39m\u001B[33m\\\u001B[39m\u001B[33melasticsearch\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;66;03m#local repo path\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Authors among files\n",
    "\n",
    "In the first distribution we can see that most Java files are modified by a single author, and the number of files decreases fast as the number of authors per file increases. Files with two authors are still relatively common, but files with three or more authors are rare. This suggests that the project follows a strong file ownership model, where most files have a clear main contributor. The few files touched by many authors are likely core or highly coupled components that require frequent coordination or are more exposed to changes during development.\n",
    "\n",
    "### Files among authors\n",
    "\n",
    "In the second image, we can see that the distribution is highly skewed. Most authors modify only a small number of files, which indicates that there are many minor or occasional contributors. In the other hand, a small number of authors modify a very large number of files, forming a long tail on the right side of the distribution. This shows a typical core–periphery structure, where a small group of core developers is responsible for most of the changes across the system, while many other contributors focus on limited and localized modifications."
   ],
   "id": "529b6c44314652e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#we define the three packages we want to analyze\n",
    "packages = [\n",
    "    \"src/main/java/org/elasticsearch/search\",\n",
    "    \"src/main/java/org/elasticsearch/index\",\n",
    "    \"src/main/java/org/elasticsearch/action\",\n",
    "]\n",
    "\n",
    "def author_contributions_for_package(package_path: str, mod_type: Modification) -> dict:\n",
    "    #we aggregate lines per author, but only for files inside the given package\n",
    "    author_to_lines = defaultdict(int)\n",
    "\n",
    "    #we iterate the commits between the two tags\n",
    "    for c in repo.iter_commits(f\"{from_tag}..{to_tag}\"):\n",
    "        author = c.author.name if c.author is not None else \"Unknown\"\n",
    "\n",
    "        #we compute per-file stats for this commit and then sum them at package level\n",
    "        for file_path, stats in c.stats.files.items():\n",
    "            #we only consider java files inside the package folder\n",
    "            if not file_path.endswith(\".java\"):\n",
    "                continue\n",
    "            if not file_path.startswith(package_path):\n",
    "                continue\n",
    "\n",
    "            lines_added = stats.get(\"insertions\", 0)\n",
    "            lines_removed = stats.get(\"deletions\", 0)\n",
    "\n",
    "            if mod_type == Modification.ADDED:\n",
    "                value = lines_added\n",
    "            elif mod_type == Modification.REMOVED:\n",
    "                value = lines_removed\n",
    "            elif mod_type == Modification.TOTAL:\n",
    "                value = lines_added + lines_removed\n",
    "            elif mod_type == Modification.DIFF:\n",
    "                value = lines_added - lines_removed\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "            author_to_lines[author] += value\n",
    "\n",
    "    return dict(author_to_lines)\n",
    "\n",
    "\n",
    "#we visualize author contributions (lines added + lines removed) for each package\n",
    "for package_path in packages:\n",
    "    author_to_total = author_contributions_for_package(package_path, Modification.TOTAL)\n",
    "\n",
    "    #we sort authors by total contribution (descending)\n",
    "    sorted_items = sorted(author_to_total.items(), key=lambda x: x[1], reverse=True)\n",
    "    authors = [a for a, _ in sorted_items]\n",
    "    totals = [v for _, v in sorted_items]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(authors, totals)\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Total lines (added + removed)\")\n",
    "    plt.title(f\"Author contributions in {package_path}\")\n",
    "    plt.xticks(rotation=60, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "721e0b9b94a2b4c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comparison of author contributions across the three packages\n",
    "\n",
    "In all three packages, the distributions show a strongly skewed contribution pattern, where a small number of authors are responsible for most of the changes, while many others contribute only a little.\n",
    "\n",
    "- org.elasticsearch.index\n",
    "This package is dominated by a single main contributor, with a very high number of modified lines compared to the rest. After the top author, there is a gradual decrease in contributions, but still a relatively large group of secondary contributors. This can indicate that the index package is actively developed and possibly complex, so it requires frequent modifications by several experienced developers, while still having a clear main maintainer.\n",
    "\n",
    "- org.elasticsearch.search\n",
    "The search package shows a slightly more balanced distribution among the top contributors. Although one author still stands out, the gap between the first few authors is smaller than in the index package. This indicates a more shared ownership among core developers, which possibly means that searching functionality is central to the system and requires collaboration and coordinated changes.\n",
    "\n",
    "- org.elasticsearch.action\n",
    "The action package presents a more fragmented contribution pattern. While there is still a leading author, the overall number of modified lines is lower, and many authors contribute small amounts. This suggests that the package may consist of more modular or isolated features, which allows occasional or task-specific contributions without a big involvement from a large core team."
   ],
   "id": "79a2ef6d4f02f91f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Knowledge loss\n",
    "\n",
    "We now want to analyze the knowledge loss when the main contributor of the analyzed project would leave. For this we will use the circle packaging layout introduced in the \"Code as a Crime Scene\" book. This assignment includes the necessary `knowledge_loss.html` file as well as the `d3` folder for all dependencies. You task is to create the `output.json` file according to the specification below. This file can then be visualized with the files provided.\n",
    "\n",
    "For showing the visualization, once you have the output as `output.json` you should\n",
    "* make sure to have the `knowledge_loss.html` file in the same folder\n",
    "* start a local HTTP server in the same folder (e.g. with python `python3 -m http.server`) to serve the html file (necessary for d3 to work)\n",
    "* open the served `knowledge_loss.html` and look at the visualization\n",
    "\n",
    "For the package you identify as the worst in terms of knowledge loss, investigate the author contributions using the function defined in the previous exercise and comment how the situation is, e.g. how big the gap between the main author and the second biggest contributor for the selected package is."
   ],
   "id": "74c182c405fcdc2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Output Format for Visualization\n",
    "\n",
    "* `root` is always the root of the tree\n",
    "* `size` should be the total number of lines of contribution\n",
    "* `weight` can be set to the same as `size`\n",
    "* `ownership` should be set to the percentage of contributions from the main author (e.g. 0.98 for 98% if contributions coming from the main author)\n",
    "\n",
    "```\n",
    "{\n",
    "  \"name\": \"root\",\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"test\",\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"benchmarking\",\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"author_color\": \"red\",\n",
    "              \"size\": \"4005\",\n",
    "              \"name\": \"t6726-patmat-analysis.scala\",\n",
    "              \"weight\": 1.0,\n",
    "              \"ownership\": 0.9,\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"author_color\": \"red\",\n",
    "              \"size\": \"55\",\n",
    "              \"name\": \"TreeSetIterator.scala\",\n",
    "              \"weight\": 0.88,\n",
    "              \"ownership\": 0.9,\n",
    "              \"children\": []\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### JSON Export\n",
    "\n",
    "For exporting the data to JSON you can use the following snippet:\n",
    "```\n",
    "import json\n",
    "\n",
    "with open(\"output.json\", \"w\") as file:\n",
    "    json.dump(tree, file, indent=4)\n",
    "```"
   ],
   "id": "25dbfff52cd0ee2d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-14T23:40:15.621138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydriller.git import Git\n",
    "\n",
    "\n",
    "class Node(BaseModel):\n",
    "    name: str\n",
    "    children: list\n",
    "\n",
    "\n",
    "class LeafNode(BaseModel):\n",
    "    name: str\n",
    "    size: int\n",
    "    author_color: str\n",
    "    weight: int\n",
    "    ownership: float\n",
    "\n",
    "\n",
    "available_colors = [\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"black\",\n",
    "    \"blue\",\n",
    "    \"cyan\",\n",
    "    \"grey\",\n",
    "    \"magenta\",\n",
    "    \"mint\",\n",
    "    \"orange\",\n",
    "    \"purple\",\n",
    "    \"white\",\n",
    "    \"yellow\",\n",
    "    \"aqua\",\n",
    "    \"beige\",\n",
    "    \"brown\",\n",
    "    \"coral\",\n",
    "    \"crimson\",\n",
    "    \"darkblue\",\n",
    "    \"darkgreen\",\n",
    "    \"darkorange\",\n",
    "    \"darkpurple\",\n",
    "    \"gold\",\n",
    "    \"indigo\",\n",
    "    \"ivory\",\n",
    "    \"khaki\",\n",
    "    \"lavender\",\n",
    "    \"lime\",\n",
    "    \"maroon\",\n",
    "    \"navy\",\n",
    "    \"olive\",\n",
    "    \"peach\",\n",
    "    \"periwinkle\",\n",
    "    \"plum\",\n",
    "    \"salmon\",\n",
    "    \"silver\",\n",
    "    \"tan\",\n",
    "    \"teal\",\n",
    "    \"turquoise\",\n",
    "    \"violet\",\n",
    "    \"wheat\",\n",
    "    \"yellowgreen\",\n",
    "    \"rose\",\n",
    "    \"sienna\",\n",
    "    \"slateblue\",\n",
    "    \"slategray\",\n",
    "    \"mediumseagreen\",\n",
    "    \"mediumvioletred\",\n",
    "    \"lightsalmon\",\n",
    "    \"lightcoral\"\n",
    "]\n",
    "\n",
    "\n",
    "def _get_new_color_for_author(data: dict) -> str:\n",
    "    if next_color := next((color for color in available_colors if color not in data.values()), None):\n",
    "        return next_color\n",
    "\n",
    "    print(\"No available colors left for new author, returning default color...\")\n",
    "    return \"red\"\n",
    "\n",
    "\n",
    "def _calculate_ownership(author_contributions: dict, total_lines: int) -> float:\n",
    "    if total_lines == 0:\n",
    "        return 0\n",
    "    main_author = max(author_contributions, key=author_contributions.get)\n",
    "    return author_contributions[main_author] / total_lines\n",
    "\n",
    "\n",
    "def _update_author_summary(data: dict, file_name: str, author_name: str) -> dict:\n",
    "    if file_name not in data:\n",
    "        data[file_name] = {}\n",
    "\n",
    "    if author_name not in data[file_name]:\n",
    "        data[file_name][author_name] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _create_new_leaf_node(file_path: str, commit, colors_map: dict, author_summary: dict) -> tuple[LeafNode, dict, dict]:\n",
    "    author_name = commit.author.name\n",
    "    file_name = file_path.split(\"/\")[-1].replace(\".java\", \"\")\n",
    "    total_modified_lines = commit.stats.files[file_path][\"deletions\"] + commit.stats.files[file_path][\"insertions\"]\n",
    "\n",
    "    author_color = colors_map.setdefault(author_name, _get_new_color_for_author(colors_map))\n",
    "\n",
    "    updated_author_summary = _update_author_summary(author_summary, file_name,  author_name)\n",
    "    updated_author_summary[file_name][author_name] += total_modified_lines\n",
    "    ownership = _calculate_ownership(updated_author_summary[file_name], total_modified_lines)\n",
    "\n",
    "    leaf_node = LeafNode(\n",
    "        name=file_name,\n",
    "        size=total_modified_lines,\n",
    "        author_color=author_color,\n",
    "        weight=total_modified_lines,\n",
    "        ownership=ownership\n",
    "    )\n",
    "\n",
    "    return leaf_node, colors_map, updated_author_summary\n",
    "\n",
    "\n",
    "def get_knowledge_map_data(commits_collection: list) -> Node:\n",
    "    authors_and_colors = {}\n",
    "    author_summary = {}\n",
    "    root = Node(name=\"root\", children=[])\n",
    "\n",
    "    for commit in commits_collection:\n",
    "        java_file_paths = [f for f in commit.stats.files.keys() if f.endswith(\".java\")]\n",
    "        for file_path in java_file_paths:\n",
    "            # add submodules as children, \"tree traversal\"\n",
    "            current = root\n",
    "            for module in file_path.split(\"/\")[:-1]:\n",
    "                if found_node := next((child for child in current.children if child.name == module), None):\n",
    "                    current = found_node\n",
    "                else:\n",
    "                    new_node = Node(name=module, children=[])\n",
    "                    current.children.append(new_node)\n",
    "                    current = new_node\n",
    "\n",
    "            # add leaf node (= file)\n",
    "            leaf_node, authors_and_colors, author_summary = _create_new_leaf_node(file_path, commit, authors_and_colors, author_summary)\n",
    "\n",
    "            if found_node := next((child for child in current.children if child.name == leaf_node.name), None):\n",
    "                found_node.size += leaf_node.size\n",
    "                # update ownership tracking\n",
    "                author_summary[leaf_node.name] = dict(sorted(author_summary[leaf_node.name].items(), key=lambda x: x[1], reverse=True))\n",
    "                found_node.ownership = _calculate_ownership(author_summary[leaf_node.name], found_node.size)\n",
    "            else:\n",
    "                current.children.append(leaf_node)\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "# checkout repo locally\n",
    "repo_url = \"https://github.com/elastic/elasticsearch\"\n",
    "repo_path = \"elasticsearch\"\n",
    "if not Path(repo_path).exists():\n",
    "    print(\"Cloning the Elasticsearch repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "from_tag = \"v1.0.0\"\n",
    "to_tag = \"v1.1.0\"\n",
    "git = Git(repo_path)  # use GitPython directly\n",
    "repo = git.repo\n",
    "commit_list = list(repo.iter_commits(f\"{from_tag}..{to_tag}\"))\n",
    "\n",
    "result = get_knowledge_map_data(commit_list)\n",
    "\n",
    "Path(\"./output.json\").open(\"w\").write(result.model_dump_json(indent=2))"
   ],
   "id": "2188bc99452094dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4bc44729fcd84941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 3: Code Churn Analysis\n",
    "\n",
    "The third and last task is to analyze the code churn of the _elasticsearch_ project. For this analysis we look at the code churn, meaning the daily change in the total number of lines of the project. Visualize the code churn over time bucketing the data by day. Remember that you'll need to interpolate the data for days when there are no commits. Chose an interpolation strategy and justify it.\n",
    "\n",
    "Look at the churn trend over time and identify two outliers. For each of them:\n",
    "- identify if it was caused by a single or multiple commits (since you are bucketing the data by day)\n",
    "- find the hash of the involved commit(s)\n",
    "- find the involved files\n",
    "- look at the actual diff\n",
    "\n",
    "Based on the above, discuss if the outlier is a false positive or should be a reason for concern."
   ],
   "id": "fcbda4e40880928e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "#we collect commit level churn information\n",
    "rows = []\n",
    "commit_range = f\"{from_tag}..{to_tag}\"\n",
    "commits = list(repo.iter_commits(commit_range))\n",
    "\n",
    "for c in commits:\n",
    "    day = pd.to_datetime(c.committed_datetime.date())\n",
    "    insertions = c.stats.total.get(\"insertions\", 0)\n",
    "    deletions = c.stats.total.get(\"deletions\", 0)\n",
    "    churn = insertions + deletions\n",
    "\n",
    "    rows.append({\n",
    "        \"day\": day,\n",
    "        \"hash\": c.hexsha,\n",
    "        \"insertions\": insertions,\n",
    "        \"deletions\": deletions,\n",
    "        \"churn\": churn\n",
    "    })\n",
    "\n",
    "commit_df = pd.DataFrame(rows)\n",
    "\n",
    "#we aggregate churn per day\n",
    "daily_churn = commit_df.groupby(\"day\")[\"churn\"].sum().sort_index()\n",
    "\n",
    "#we create a full daily timeline and fill missing days with 0 churn\n",
    "#we choose this interpolation because churn is defined by commits.\n",
    "#no commits on a day means no recorded code change\n",
    "full_days = pd.date_range(daily_churn.index.min(), daily_churn.index.max(), freq=\"D\")\n",
    "daily_churn = daily_churn.reindex(full_days).fillna(0)\n",
    "\n",
    "#we visualize the daily churn trend\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(daily_churn.index, daily_churn.values)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Daily code churn (insertions + deletions)\")\n",
    "plt.title(\"Elasticsearch daily code churn\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#we identify the two largest churn outliers\n",
    "outlier_days = daily_churn.sort_values(ascending=False).head(2)\n",
    "print(\"Top 2 churn outlier days:\")\n",
    "print(outlier_days)\n",
    "print()\n",
    "\n",
    "#we analyze each outlier day in detail\n",
    "for day in outlier_days.index:\n",
    "    day_date = day.date()\n",
    "    day_commits = commit_df[commit_df[\"day\"].dt.date == day_date].copy()\n",
    "    day_commits = day_commits.sort_values(\"churn\", ascending=False)\n",
    "\n",
    "    print(\"=\" * 90)\n",
    "    print(\"OUTLIER DAY:\", day_date)\n",
    "    print(\"total churn:\", int(daily_churn.loc[day]))\n",
    "    print(\"number of commits:\", len(day_commits))\n",
    "    print()\n",
    "\n",
    "    for _, row in day_commits.iterrows():\n",
    "        h = row[\"hash\"]\n",
    "        print(\"commit:\", h)\n",
    "        print(\n",
    "            \"insertions:\", int(row[\"insertions\"]),\n",
    "            \"| deletions:\", int(row[\"deletions\"]),\n",
    "            \"| churn:\", int(row[\"churn\"])\n",
    "        )\n",
    "\n",
    "        c = repo.commit(h)\n",
    "        files = list(c.stats.files.keys())\n",
    "\n",
    "        print(\"number of files touched:\", len(files))\n",
    "        print(\"files (first 25):\")\n",
    "        for f in files[:25]:\n",
    "            print(\" \", f)\n",
    "        if len(files) > 25:\n",
    "            print(\"  ...\")\n",
    "\n",
    "        #we show a truncated diff to keep output readable\n",
    "        print(\"\\nDIFF (truncated):\")\n",
    "        diff_text = repo.git.show(h)\n",
    "        diff_lines = diff_text.splitlines()\n",
    "        print(\"\\n\".join(diff_lines[:120]))\n",
    "        if len(diff_lines) > 120:\n",
    "            print(\"... (diff truncated)\")\n",
    "        print()\n"
   ],
   "id": "d1ee96a0d6d73a54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Outlier analysis: is it a false positive or a reason for concern?\n",
    "\n",
    "- **Outlier at 2014-02-26:** \n",
    "\n",
    "The strongest outlier happens at 2014-02-26, with a daily churn of 9,661 lines. It clearly stands out from the surrounding days in the churn trend. At first glance, this spike might suggest a risky or problematic change. However, if we look closely we can see that this outlier is not a reason for concern.\n",
    "\n",
    "First, the churn on this day is caused by multiple commits (16 commits) rather than a single unusually large commit. This indicates a coordinated development activity rather than an uncontrolled or accidental change.\n",
    "\n",
    "Second, the largest contributing commit touches a large number of files (72 files), spread across several core packages such as common.util, cache, and index.cache. This pattern is typical of a systematic refactoring or architectural change, and not a localized bug fix or a rushed feature addition.\n",
    "\n",
    "Looking at the file names and packages involved, the changes appear to be related to memory management and internal infrastructure. These components are central to Elasticsearch’s performance and often require broad updates to maintain consistency and efficiency.\n",
    "\n",
    "Finally, the churn consists of both insertions and deletions in comparable magnitude, which is a characteristic of a refactor rather than an uncontrolled code growth.\n",
    "\n",
    "Conclusion:\n",
    "This outlier should be considered a false positive in terms of risk detection. Although the churn is high, the changes are distributed across many files, occur in multiple commits, and are consistent with a deliberate and coordinated refactoring. So, it indicate healthy maintenance activity rather than a problematic development event.\n",
    "\n",
    "- **Outlier at 2014-03-13**\n",
    "\n",
    "The second outlier happens on 2014-03-13, with a total churn of 7,968 lines. Like the previous outlier, this spike is not caused by a single massive commit, but by multiple commits (18 commits) aggregated on the same day. That already suggests this is a “busy development day” rather than an isolated suspicious event.\n",
    "\n",
    "When looking at some of the biggest contributing commits, the reason for the churn becomes clear:\n",
    "    One commit introduces the \"Context Suggester feature\", where the diff shows a large amount of new documentation. Another commit introduces a \"Cardinality aggregation feature\". The addition of this features is planned, therefore, this outlier is also a false positive of risk detection."
   ],
   "id": "de81339bfbbca51f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interpolation justification: \n",
    "Days without commits are filled with 0 churn. Since churn is defined as changes introduced by commits, no commits implies no recorded churn for that day."
   ],
   "id": "59f222f9928e5b4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
