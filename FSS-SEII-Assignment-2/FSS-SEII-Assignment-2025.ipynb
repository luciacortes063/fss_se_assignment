{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fundamentals of Software Systems (FSS)\n",
    "**Software Evolution â€“ Part 02 Assignment**\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "To correctly complete this assignment you must:\n",
    "* Carry out the assignment in a team of 2 to 4 students.\n",
    "* Carry out the assignment with your team only. You are allowed to discuss solutions with other teams, but each team should come up its own personal solution. A strict plagiarism policy is going to be applied to all the artifacts submitted for evaluation.\n",
    "* As your submission, upload the filled Jupyter Notebook (including outputs) together with the d3 visualization web pages (i.e. upload everything you downloaded including the filled Jupyter Notebook plus your `output.json`)\n",
    "* The files must be uploaded to OLAT as a single ZIP (`.zip`) file by Dec 15, 2025 @ 23:55.\n"
   ],
   "id": "e2c8bdd91f51401c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Group Members\n",
    "* Lucia, TODO, TODO\n",
    "* Oliver, Strassmann, 15-932-726"
   ],
   "id": "e8956f7a56c37748"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task Context\n",
    "\n",
    "In this assigment we will be analyzing the _elasticsearch_ project. All following tasks should be done with the subset of commits from tag `v1.0.0` to tag `v1.1.0`."
   ],
   "id": "816bcad839d94ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T01:17:20.075385Z",
     "start_time": "2025-12-13T01:17:20.071233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pydriller\n",
    "import subprocess\n",
    "\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Modification(Enum):\n",
    "    ADDED = \"Lines added\"\n",
    "    REMOVED = \"Lines removed\"\n",
    "    TOTAL = \"Lines added + lines removed\"\n",
    "    DIFF = \"Lines added - lines removed\"\n",
    "\n",
    "\n",
    "repo_url = \"https://github.com/elastic/elasticsearch\"\n",
    "repo_path = \"elasticsearch\"\n",
    "\n",
    "if not Path(repo_path).exists():\n",
    "    print(\"Cloning the Elasticsearch repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "\n",
    "repo = pydriller.Repository(repo_path, from_tag=\"v1.0.0\", to_tag=\"v1.1.0\")"
   ],
   "id": "5309f0bae7fc2963",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Author analysis\n",
    "\n",
    "In the following, please consider only `java` files.\n",
    "\n",
    "The first task is to get an overview of the author ownership of the _elasticsearch_ project. In particular, we want to understand who are the main authors in the system between the two considered tags, the authors distribution among files and the files distribution among authors. To this aim, perform the following:\n",
    "- create a dictionary (or a list of tuples) with the pairs author => number of modified files\n",
    "- create a dictionary (or a list of tuples) with the pairs file => number of authors who modified the file\n",
    "- visualize the distribution of authors among files: the visualization should have on the x axis the number of authors per file (from 1 to max), and on the y axis the number of files with the given number of authors (so for example the first bar represent the number of files with single author)\n",
    "- visualize the distribution of files among authors: the visualization should have on the x axis the number of files per author (from 1 to max), and on the y axis the number of authors that own the given number of files (so for example the first bar represent the minor contributors, i.e., the number of authors who own 1 file)\n",
    "\n",
    "Comment the two distribution visualizations.\n",
    "\n",
    "\n",
    "\n",
    "Now, let's look at the following 3 packages in more details:\n",
    "1. `src/main/java/org/elasticsearch/search`\n",
    "2. `src/main/java/org/elasticsearch/index`\n",
    "3. `src/main/java/org/elasticsearch/action`\n",
    "\n",
    "Create a function that, given the path of a package and a modification type (see class Modification above), returns a dictionary of authors => number, where the number counts the total lines added or removed or added+removed or added-removed (depending on the given Modification parameter), for the given package. To compute the value at the package level, you should aggregate the data per file.\n",
    "\n",
    "Using the function defined above, visualize the author contributions (lines added + lines removed). The visualization should have the author on the x axis, and the total lines on the y axis. Sort the visualization in decreasing amount of contributions, i.e., the main author should be the first.\n",
    "\n",
    "Compare the visualization for the 3 packages and comment."
   ],
   "id": "edde08570ea4ee3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T01:17:20.180172Z",
     "start_time": "2025-12-13T01:17:20.178018Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "86cacff0807258d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T01:17:20.212587Z",
     "start_time": "2025-12-13T01:17:20.210501Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2d38d271f11ac90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Knowledge loss\n",
    "\n",
    "We now want to analyze the knowledge loss when the main contributor of the analyzed project would leave. For this we will use the circle packaging layout introduced in the \"Code as a Crime Scene\" book. This assignment includes the necessary `knowledge_loss.html` file as well as the `d3` folder for all dependencies. You task is to create the `output.json` file according to the specification below. This file can then be visualized with the files provided.\n",
    "\n",
    "For showing the visualization, once you have the output as `output.json` you should\n",
    "* make sure to have the `knowledge_loss.html` file in the same folder\n",
    "* start a local HTTP server in the same folder (e.g. with python `python3 -m http.server`) to serve the html file (necessary for d3 to work)\n",
    "* open the served `knowledge_loss.html` and look at the visualization\n",
    "\n",
    "For the package you identify as the worst in terms of knowledge loss, investigate the author contributions using the function defined in the previous exercise and comment how the situation is, e.g. how big the gap between the main author and the second biggest contributor for the selected package is."
   ],
   "id": "7d19987a6e1fa29e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Output Format for Visualization\n",
    "\n",
    "* `root` is always the root of the tree\n",
    "* `size` should be the total number of lines of contribution\n",
    "* `weight` can be set to the same as `size`\n",
    "* `ownership` should be set to the percentage of contributions from the main author (e.g. 0.98 for 98% if contributions coming from the main author)\n",
    "\n",
    "```\n",
    "{\n",
    "  \"name\": \"root\",\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"test\",\n",
    "      \"children\": [\n",
    "        {\n",
    "          \"name\": \"benchmarking\",\n",
    "          \"children\": [\n",
    "            {\n",
    "              \"author_color\": \"red\",\n",
    "              \"size\": \"4005\",\n",
    "              \"name\": \"t6726-patmat-analysis.scala\",\n",
    "              \"weight\": 1.0,\n",
    "              \"ownership\": 0.9,\n",
    "              \"children\": []\n",
    "            },\n",
    "            {\n",
    "              \"author_color\": \"red\",\n",
    "              \"size\": \"55\",\n",
    "              \"name\": \"TreeSetIterator.scala\",\n",
    "              \"weight\": 0.88,\n",
    "              \"ownership\": 0.9,\n",
    "              \"children\": []\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### JSON Export\n",
    "\n",
    "For exporting the data to JSON you can use the following snippet:\n",
    "```\n",
    "import json\n",
    "\n",
    "with open(\"output.json\", \"w\") as file:\n",
    "    json.dump(tree, file, indent=4)\n",
    "```"
   ],
   "id": "ce8e45cd3d3094e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T01:25:02.503223Z",
     "start_time": "2025-12-13T01:24:07.641443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "from pydriller import Repository, ModifiedFile\n",
    "\n",
    "\n",
    "class Node(BaseModel):\n",
    "    name: str\n",
    "    children: list\n",
    "\n",
    "\n",
    "class LeafNode(BaseModel):\n",
    "    name: str\n",
    "    size: int\n",
    "    author_color: str\n",
    "    weight: int\n",
    "    ownership: float\n",
    "\n",
    "\n",
    "available_colors = [\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"black\",\n",
    "    \"blue\",\n",
    "    \"cyan\",\n",
    "    \"grey\", \n",
    "    \"magenta\", \n",
    "    \"mint\", \n",
    "    \"orange\", \n",
    "    \"purple\",\n",
    "    \"white\", \n",
    "    \"yellow\"\n",
    "]\n",
    "\n",
    "test_contributions = [0.5, 0.8, 0.75, 0.9, 0.33, 0.450, 0.99]\n",
    "\n",
    "\n",
    "def _get_new_color_for_author(data: dict) -> str:\n",
    "    if next_color := next((color for color in available_colors if color not in data.values()), None):\n",
    "        return next_color\n",
    "\n",
    "    print(\"No available colors left for new author, returning default color...\")\n",
    "    return \"red\"\n",
    "\n",
    "\n",
    "def _get_main_author_contributions() -> float:\n",
    "    # TODO: implement actual ownership calculation\n",
    "    return random.choice(test_contributions)\n",
    "\n",
    "\n",
    "def _create_new_leaf_node(file: ModifiedFile, author_name: str, colors_map: dict) -> tuple[LeafNode, dict]:\n",
    "    total_modified_lines = file.added_lines + file.deleted_lines\n",
    "    author_color = colors_map.setdefault(author_name, _get_new_color_for_author(colors_map))\n",
    "\n",
    "    return LeafNode(\n",
    "        name=file.filename.replace(\".java\", \"\"),\n",
    "        size=total_modified_lines,\n",
    "        author_color=author_color,\n",
    "        weight=total_modified_lines,\n",
    "        ownership=_get_main_author_contributions()\n",
    "    ), colors_map\n",
    "\n",
    "\n",
    "def get_knowledge_map_data(git_repo_path: str) -> Node:\n",
    "    repo_obj = Repository(str(git_repo_path))\n",
    "    authors_and_colors = {}\n",
    "    root = Node(name=\"root\", children=[])\n",
    "    max_commits = 1000\n",
    "\n",
    "    for commit in repo_obj.traverse_commits():\n",
    "        if max_commits <= 0:\n",
    "            break\n",
    "\n",
    "        if java_files := [f for f in commit.modified_files if f.filename.endswith(\".java\")]:\n",
    "            for file in java_files:\n",
    "                current_node = root\n",
    "\n",
    "                # add sub-modules as children\n",
    "                parts = file.new_path.split(\"/\") if file.new_path else file.old_path.split(\"/\")\n",
    "                for sub_module in parts[:-1]:\n",
    "                    found_child = next((child for child in current_node.children if child.name == sub_module), None)\n",
    "\n",
    "                    if not found_child:\n",
    "                        new_node = Node(name=sub_module, children=[])\n",
    "                        current_node.children.append(new_node)\n",
    "                        current_node = new_node\n",
    "                    else:\n",
    "                        current_node = found_child\n",
    "\n",
    "                # TODO: If node already exists sum up total line count\n",
    "                leaf_node, authors_and_colors = _create_new_leaf_node(file, commit.author.name, authors_and_colors)\n",
    "                current_node.children.append(leaf_node)\n",
    "\n",
    "        max_commits -= 1\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "result = get_knowledge_map_data(repo_path)\n",
    "\n",
    "Path(\"./output.json\").open(\"w\").write(result.model_dump_json(indent=2))"
   ],
   "id": "456045d7f73fd8fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5251143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T01:17:28.785327Z",
     "start_time": "2025-12-13T01:17:28.783215Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "52e29c7b15a2b27c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 3: Code Churn Analysis\n",
    "\n",
    "The third and last task is to analyze the code churn of the _elasticsearch_ project. For this analysis we look at the code churn, meaning the daily change in the total number of lines of the project. Visualize the code churn over time bucketing the data by day. Remember that you'll need to interpolate the data for days when there are no commits. Chose an interpolation strategy and justify it.\n",
    "\n",
    "Look at the churn trend over time and identify two outliers. For each of them:\n",
    "- identify if it was caused by a single or multiple commits (since you are bucketing the data by day)\n",
    "- find the hash of the involved commit(s)\n",
    "- find the involved files\n",
    "- look at the actual diff\n",
    "\n",
    "Based on the above, discuss if the outlier is a false positive or should be a reason for concern."
   ],
   "id": "462c69109fbf8ce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T01:17:28.802532Z",
     "start_time": "2025-12-13T01:17:28.800602Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "84421d784866b11b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T01:17:28.814589Z",
     "start_time": "2025-12-13T01:17:28.812861Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d33075a7a0575acd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
